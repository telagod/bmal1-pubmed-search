#!/usr/bin/env python3
# Archived: superseded by pubmed_search_v2.py. Not used by app.
"""
BMAL1æ–‡çŒ®æ£€ç´¢è„šæœ¬
ä½¿ç”¨PubMed APIè¿›è¡Œæ–‡çŒ®æ£€ç´¢å¹¶ä¿å­˜ç»“æœ
"""

import os
from pathlib import Path
from datetime import datetime
from Bio import Entrez
import json


def load_env():
    """ä».envæ–‡ä»¶åŠ è½½é…ç½®"""
    env_path = Path(__file__).parent.parent / ".env"
    config = {}
    with open(env_path) as f:
        for line in f:
            line = line.strip()
            if line and not line.startswith("#"):
                key, value = line.split(":", 1)
                config[key.strip()] = value.strip()
    return config


def search_pubmed(query, email, api_key, max_results=100):
    """
    åœ¨PubMedä¸­æœç´¢æ–‡çŒ®

    å‚æ•°:
        query: æœç´¢å…³é”®è¯
        email: PubMed APIé‚®ç®±
        api_key: PubMed APIå¯†é’¥
        max_results: æœ€å¤§è¿”å›ç»“æœæ•°

    è¿”å›:
        æ–‡çŒ®IDåˆ—è¡¨å’Œæœç´¢ç»Ÿè®¡
    """
    Entrez.email = email
    Entrez.api_key = api_key

    print(f"\nğŸ” æ­£åœ¨æœç´¢å…³é”®è¯: {query}")
    print(f"ğŸ“Š æœ€å¤§è¿”å›ç»“æœæ•°: {max_results}")

    # æ‰§è¡Œæœç´¢
    handle = Entrez.esearch(
        db="pubmed",
        term=query,
        retmax=max_results,
        sort="relevance"
    )

    results = Entrez.read(handle)
    handle.close()

    id_list = results["IdList"]
    count = int(results["Count"])

    print(f"âœ… æ‰¾åˆ° {count} ç¯‡ç›¸å…³æ–‡çŒ®")
    print(f"ğŸ“¥ è·å–å‰ {len(id_list)} ç¯‡æ–‡çŒ®ä¿¡æ¯")

    return id_list, count


def fetch_details(id_list, email, api_key, batch_size=20):
    """
    è·å–æ–‡çŒ®è¯¦ç»†ä¿¡æ¯

    å‚æ•°:
        id_list: æ–‡çŒ®IDåˆ—è¡¨
        email: PubMed APIé‚®ç®±
        api_key: PubMed APIå¯†é’¥
        batch_size: æ‰¹é‡è·å–å¤§å°

    è¿”å›:
        æ–‡çŒ®è¯¦ç»†ä¿¡æ¯åˆ—è¡¨
    """
    Entrez.email = email
    Entrez.api_key = api_key

    all_papers = []

    # åˆ†æ‰¹è·å–
    for i in range(0, len(id_list), batch_size):
        batch_ids = id_list[i:i+batch_size]
        print(f"ğŸ“– æ­£åœ¨è·å–ç¬¬ {i+1}-{min(i+batch_size, len(id_list))} ç¯‡æ–‡çŒ®è¯¦æƒ…...")

        handle = Entrez.efetch(
            db="pubmed",
            id=batch_ids,
            rettype="xml",
            retmode="xml"
        )

        records = Entrez.read(handle)
        handle.close()

        # æå–å…³é”®ä¿¡æ¯
        for record in records['PubmedArticle']:
            try:
                article = record['MedlineCitation']['Article']

                paper_info = {
                    'pmid': str(record['MedlineCitation']['PMID']),
                    'title': article['ArticleTitle'],
                    'abstract': article.get('Abstract', {}).get('AbstractText', [''])[0] if 'Abstract' in article else '',
                    'journal': article['Journal']['Title'],
                    'pub_date': '',
                    'authors': [],
                    'keywords': []
                }

                # è·å–å‘è¡¨æ—¥æœŸ
                if 'PubDate' in article['Journal']['JournalIssue']:
                    pub_date = article['Journal']['JournalIssue']['PubDate']
                    year = pub_date.get('Year', '')
                    month = pub_date.get('Month', '')
                    paper_info['pub_date'] = f"{year}-{month}" if month else year

                # è·å–ä½œè€…
                if 'AuthorList' in article:
                    for author in article['AuthorList'][:5]:  # åªå–å‰5ä¸ªä½œè€…
                        if 'LastName' in author and 'Initials' in author:
                            paper_info['authors'].append(
                                f"{author['LastName']} {author['Initials']}"
                            )

                # è·å–å…³é”®è¯
                if 'KeywordList' in record['MedlineCitation']:
                    paper_info['keywords'] = [
                        str(kw) for kw in record['MedlineCitation']['KeywordList'][0][:10]
                    ]

                all_papers.append(paper_info)
            except Exception as e:
                print(f"âš ï¸ å¤„ç†æ–‡çŒ®æ—¶å‡ºé”™: {e}")
                continue

    return all_papers


def save_results(papers, query, output_dir):
    """
    ä¿å­˜æ£€ç´¢ç»“æœ

    å‚æ•°:
        papers: æ–‡çŒ®ä¿¡æ¯åˆ—è¡¨
        query: æœç´¢å…³é”®è¯
        output_dir: è¾“å‡ºç›®å½•
    """
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    # ä¿å­˜JSONæ ¼å¼
    json_file = output_dir / f"bmal1_search_{timestamp}.json"
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump({
            'query': query,
            'timestamp': timestamp,
            'total_papers': len(papers),
            'papers': papers
        }, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ’¾ JSONç»“æœå·²ä¿å­˜è‡³: {json_file}")

    # ä¿å­˜Markdownæ ¼å¼
    md_file = output_dir / f"bmal1_search_{timestamp}.md"
    with open(md_file, 'w', encoding='utf-8') as f:
        f.write(f"# BMAL1æ–‡çŒ®æ£€ç´¢ç»“æœ\n\n")
        f.write(f"**æ£€ç´¢æ—¶é—´**: {timestamp}\n")
        f.write(f"**æ£€ç´¢å…³é”®è¯**: {query}\n")
        f.write(f"**æ–‡çŒ®æ•°é‡**: {len(papers)}\n\n")
        f.write("---\n\n")

        for idx, paper in enumerate(papers, 1):
            f.write(f"## {idx}. {paper['title']}\n\n")
            f.write(f"**PMID**: {paper['pmid']}\n")
            f.write(f"**æœŸåˆŠ**: {paper['journal']}\n")
            f.write(f"**å‘è¡¨æ—¥æœŸ**: {paper['pub_date']}\n")

            if paper['authors']:
                f.write(f"**ä½œè€…**: {', '.join(paper['authors'])}\n")

            if paper['keywords']:
                f.write(f"**å…³é”®è¯**: {', '.join(paper['keywords'])}\n")

            if paper['abstract']:
                f.write(f"\n**æ‘˜è¦**:\n{paper['abstract']}\n")

            f.write(f"\n**PubMedé“¾æ¥**: https://pubmed.ncbi.nlm.nih.gov/{paper['pmid']}/\n")
            f.write("\n---\n\n")

    print(f"ğŸ“„ Markdownç»“æœå·²ä¿å­˜è‡³: {md_file}")

    return json_file, md_file


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ğŸ§¬ BMAL1æ–‡çŒ®æ£€ç´¢å·¥å…·")
    print("=" * 60)

    # åŠ è½½é…ç½®
    config = load_env()
    email = config.get('pubmed_email')
    api_key = config.get('api_key')

    if not email or not api_key:
        print("âŒ é”™è¯¯: æœªæ‰¾åˆ°é‚®ç®±æˆ–APIå¯†é’¥")
        return

    print(f"ğŸ“§ é‚®ç®±: {email}")
    print(f"ğŸ”‘ APIå¯†é’¥: {api_key[:10]}...")

    # å®šä¹‰æœç´¢ç­–ç•¥
    queries = [
        "BMAL1 AND (circadian OR clock)",
        "BMAL1 AND Alzheimer",
        "BMAL1 AND (glymphatic OR clearance)",
        "BMAL1 AND (astrocyte OR BBB OR blood-brain barrier)"
    ]

    output_dir = Path(__file__).parent / "results"
    output_dir.mkdir(exist_ok=True)

    all_results = {}

    for query in queries:
        print(f"\n{'='*60}")
        print(f"æœç´¢ç­–ç•¥: {query}")
        print(f"{'='*60}")

        # æœç´¢æ–‡çŒ®
        id_list, total_count = search_pubmed(query, email, api_key, max_results=50)

        if not id_list:
            print("âš ï¸ æœªæ‰¾åˆ°ç›¸å…³æ–‡çŒ®")
            continue

        # è·å–è¯¦æƒ…
        papers = fetch_details(id_list, email, api_key)

        # ä¿å­˜ç»“æœ
        json_file, md_file = save_results(papers, query, output_dir)

        all_results[query] = {
            'total_count': total_count,
            'fetched_count': len(papers),
            'json_file': str(json_file),
            'md_file': str(md_file)
        }

    # ä¿å­˜æ€»ç»“
    summary_file = output_dir / f"search_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(all_results, f, ensure_ascii=False, indent=2)

    print(f"\n{'='*60}")
    print("âœ… æ‰€æœ‰æ£€ç´¢å®Œæˆï¼")
    print(f"ğŸ“Š æ£€ç´¢æ‘˜è¦å·²ä¿å­˜è‡³: {summary_file}")
    print(f"{'='*60}\n")


if __name__ == "__main__":
    main()
